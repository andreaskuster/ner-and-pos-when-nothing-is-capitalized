%
% File emnlp2019.tex
%
%% Based on the style files for ACL 2019, which were
%% Based on the style files for EMNLP 2018, which were
%% Based on the style files for ACL 2018, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{emnlp-ijcnlp-2019}
\usepackage{hyperref}
\usepackage{times}
\usepackage{latexsym}

%\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}
\newcommand\confname{EMNLP-IJCNLP 2019}
\newcommand\conforg{SIGDAT}

\title{reproducing ``ner and pos when nothing is capitalized"}

\author{Andreas Kuster \\
    {\tt kustera@student.ethz} \\\And
    Jakub Filipek \\
    {\tt balbok@uw} \\\And
    Viswa Virinchi Muppirala \\
    {\tt virinchi@uw}}
\date{}

\makeatletter
\def\endthebibliography{%
	\def\@noitemerr{\@latex@warning{Empty `thebibliography' environment}}%
	\endlist
}
\makeatother

\begin{document}
\maketitle
\begin{abstract}
    Capitalization is an important feature in many NLP tasks such as Named Entity Recognition (NER) or Part of Speech Tagging (POS). We are trying to reproduce results of paper which shows how to mitigate a significant performance drop when casing is mismatched between training and testing data. In particular we show that mixing cased and uncased dataset provides the best performance, matching the claims of the original paper. We also show that we got slightly lower performance in almost all experiments we have tried to reproduce, suggesting, that the orginal paper did not fully disclose all of the experimental setup.
\end{abstract}

\section{Introduction}
Previous works have shown that there is a significant performance drop when applying models trained on cased data to uncased data and vice-versa \textcolor{red}{(citation needed)}. Since capitalization is not always available due to real world constraints, there have been some methods trying to use casing prediction (called \textit{truecasing}) to battle this trade-off.

The work we reproduce tries to battle this issue in two popular NLP tasks by mixing both cased and uncased datasets.

\section{Contributions}
This paper effectively shows how well work from \cite{ner-and-pos-original} can be reproducede and how well it applies to a few other settings.\\
Original paper does show how casing issue in NLP can be effectively solved through a method which requires close to none overhead in terms of development time, and no additional overhead in runtime, especially when compared to methods such as truecasing.\\
It also serves as a reproducing work to show that truecasing can be reproduced. We show that while truecasing experiment can be reproduced on the same experiment, but claims on applicability to other datasets fail to reproduce.

\subsection{Hypotheses from original paper}
\label{sec:original_hypotheses}
Original paper proposes following hypotheses:
\begin{itemize}
    \item Truecasing fits strongly to data it is presented leading to performance drops on different datasets.
    \item Mixing cased and uncased data provides the best performance in NER task on CoNLL 2003 dataset.
    \item Such techinque generalizes well on noisy datasets such as Twitter data.
    \item Mixing cased and uncased data provides the best performance in POS task on Penn Treebank dataset.
\end{itemize}

\subsection{Hypotheses addressed in this work}
\label{sec:current_hypotheses}
In addition to hypotheses tested in Section~\ref{sec:original_hypotheses} we also tested:
\begin{itemize}
    \item POS:
    \begin{itemize}
        \item Mixing both cased and uncased data leads to the best performance regardless of word embedding and dataset used.
        \item Using ELMo model with CRF layer in POS task to outperform the one without, regardless of data casing technique. This is based on \cite{BiLSTM-CRF}.
    \end{itemize}
    \item NER:
    \begin{itemize}
        \item NER task is transferable to other, similar dataset, such as Groningen Meaning Bank (GMB) in this case.
    \end{itemize}
\end{itemize}


\subsection{Experiments}
We conducted two main experiments from the paper, and tried to confirm the reproducibility claim that the original paper made.

For the truecasing we are training a simple Bidirectional LSTM with a logical layer on top, as described in both \cite{ner-and-pos-original} and \cite{susanto-etal-2016-learning}. Since the former paper, does a great job of mentioning hyper-parameters used in the network, the hyperparameter search is not required. However, due to a mistake which was pointed out on feedback of version 1 of this project, the Out Of Vocabulary (OOV) rate was non-standard during training. Due to this fact, we actually get 2 separate results, each for different type of determining whether a token is OOV during training or not. Changing of such technique has an effect on model parameters, which will be discussed in Section \ref{sec:exp-truecase}.

Part of Speech tagging is the most in depth experiment, which lead a lot of additional hypotheses which got tested. Firstly, the \textbf{\textcolor{red}{TO BE DONE}}

Named Entity Recognition \textbf{\textcolor{red}{TO BE DONE}}

Here every experiment is listed briefly.
\textbf{Every} hypothesis in Section~\ref{sec:current_hypotheses} should be listed and supported by at least one experiment, plot, table, or other type of data. 
Every piece of data should be listed here, with the hypothesis it supports.

\section{Code}
There are no public repos which do mention the project in their \textit{READMEs}.

However, in case of truecasing, primary author of \cite{ner-and-pos-original} has two repositories: \href{https://github.com/mayhewsw/truecaser}{truecaser} and \href{https://github.com/mayhewsw/pytorch-truecaser}{python-truecaser}. The former one refers to the original implementation from \cite{susanto-etal-2016-learning}, while the python one is a port to python. The (probably) primary author of \cite{susanto-etal-2016-learning} also has a \href{https://github.com/raymondhs/char-rnn-truecase}{github repository} possibly related to truecaser we are trying to reproduce. However, to validate results and test ease of application we decided to not use either of these resources and focus on custom implementation. This also removes dependence on Andrej Karpathy's \href{https://github.com/karpathy/char-rnn}{char-rnn} all of the above mentioned repositories are forks of.

We were not able to find publicly available code for either NER or POS parts of the original paper.

Hence we needed to reimplement the code from scratch. All our work is in \href{https://github.com/andreaskuster/uw-nlp}{public github repository}. We tried to make all results as easily accessible as possible, which means that there is significant overlap between this report and the \textit{README} on that repository.

\section{Experimental setup and results}

\subsection{Datasets}
Datasets were a common resource about all three parts of experiments. Hence we will describe them separate here, and for each experiment specify which exact dataset was used:
\begin{itemize}
    \item Brown
    \item CoNLL2000
    \item Peen Tree Bank
    \item Twitter
    \item Wikipedia - we used data from \href{https://github.com/raymondhs/char-rnn-truecase/tree/master/data/wiki}{github repository}, which we expect is authored by primary author of \cite{susanto-etal-2016-learning}.
\end{itemize}


CoNLL2000

CoNLL2003

Peen Tree Bank

Twitter

Wikipedia


\subsection{Truecasing}
\label{sec:exp-truecase}

    \subsubsection{Model description}
    We used the exact same model as described in \cite{susanto-etal-2016-learning}, a 2 layer, bidirectional LSTM. Since encoding was not specified, nor any character level encoding was mentioned in class we used a PyTorch \cite{pytorch} Encoder layer, which learned weights from bag-of-words to a feature-sized word representation. Note that in this model size of word embeddings is equal to hidden state size (both 300 dimensional). Then on top of these two layer a binary linear layer was applied.

    Hence output of this model is 2 dimensional, one specifying that character should be upper cased (true), other that character should be left as is.

    Implementation of this model took 2 hours, mostly due to transposes required by LSTM layers in PyTorch framework. It also helped that homework 1

    \subsubsection{Hyperparameters}
    See syllabus. Describe what you can.

    \subsubsection{Results}
    Each experiment should have: a clear explanation of how it was run, the high-level takeaway, a pointer to the hypothesis it supports, and it should say whether it reproduces the experiments in the original paper or not.

\subsection{Part of Speech Tagging}
\label{sec:exp-pos}

    \subsubsection{Model description}
    See syllabus. If you had to implement the model, record how long each part took.

    \subsubsection{Hyperparameters}
    See syllabus. Describe what you can.

    \subsubsection{Results}
    Each experiment should have: a clear explanation of how it was run, the high-level takeaway, a pointer to the hypothesis it supports, and it should say whether it reproduces the experiments in the original paper or not.

\subsection{Named Entity Recognition}
\label{sec:exp-ner}

    \subsubsection{Model description}
    See syllabus. If you had to implement the model, record how long each part took.

    \subsubsection{Hyperparameters}
    See syllabus. Describe what you can.

    \subsubsection{Results}
    Each experiment should have: a clear explanation of how it was run, the high-level takeaway, a pointer to the hypothesis it supports, and it should say whether it reproduces the experiments in the original paper or not.

\section{Experiments beyond the original paper}
See syllabus.

\section{Computational requirements}
Due to relative lack of overlap between three sections each of us used different computational resources:
\begin{itemize}
    \item Truecasing:
    \begin{itemize}
        \item GPU: NVIDIA RTX 2080 Super
        \item CPU: AMD Ryzen 7 3700x
        \item Runtime: 10h (Original Paper + Debugging)
    \end{itemize}
    \item POS:
    \begin{itemize}
        \item GPU: 2x NVIDIA V100
        \item CPU: AMD EPYC 7501
        \item Runtime: 15h (Original Paper + Additional Experiments + Debugging)
    \end{itemize}
    \item NER:
    \begin{itemize}
        \item GPU: \textbf{\textcolor{red}{TO BE FILLED}}
        \item CPU: \textbf{\textcolor{red}{TO BE FILLED}}
        \item Runtime: \textbf{\textcolor{red}{TO BE FILLED}}
    \end{itemize}
\end{itemize}

This is required for every report. Include every item listed in the syllabus, plus any other relevant statistics (e.g. if you have multiple model sizes, report info for each).
Include the information listed in the syllabus, including the \textbf{total} number of GPU hours used for all experiments, and the number of GPU hours for \textbf{each} experiment.

\section{Discussion and recommendations}
Conclude the report, and summarize which hypotheses from the original paper were reproducible. Include suggestions for future researchers -- what was hard? What worked easily? 

\bibliographystyle{acl_natbib}
\bibliography{emnlp-ijcnlp-2019}

\end{document}
